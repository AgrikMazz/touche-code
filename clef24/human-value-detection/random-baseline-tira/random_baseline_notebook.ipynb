{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Random-Baseline model from ACL'22"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple\n",
    "import random\n",
    "\n",
    "runs_as_inference_server = os.environ.get('TIRA_INFERENCE_SERVER', None) is not None\n",
    "dataset_dir = os.environ.get('TIRA_INPUT_DIRECTORY', './dataset')\n",
    "output_dir = os.environ.get('TIRA_OUTPUT_DIRECTORY', './output')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "values = [ \"Self-direction: thought\", \"Self-direction: action\", \"Stimulation\", \"Hedonism\", \"Achievement\", \"Power: dominance\", \"Power: resources\", \"Face\", \"Security: personal\", \"Security: societal\", \"Tradition\", \"Conformity: rules\", \"Conformity: interpersonal\", \"Humility\", \"Benevolence: caring\", \"Benevolence: dependability\", \"Universalism: concern\", \"Universalism: nature\", \"Universalism: tolerance\" ]\n",
    "probabilities_resorted = [ 0.17, 0.26, 0.06, 0.04, 0.27, 0.09, 0.11, 0.07, 0.38, 0.31, 0.11, 0.23, 0.04, 0.08, 0.29, 0.15, 0.38, 0.07, 0.14 ]\n",
    "probabilities_attained = [ 0.5 for _ in range(len(values)) ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predict function"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def choose_values_by_probability(prob_resort: float, prob_attained: float) -> Tuple[float, float]:\n",
    "    if random.random() > prob_resort:\n",
    "        return 0.0, 0.0\n",
    "    if random.random() > prob_attained:\n",
    "        return 0.0, 1.0\n",
    "    return 1.0, 0.0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compatibility function for running as inference server\n",
    "def predict(input_list: List) -> List[Dict]:\n",
    "    if not runs_as_inference_server:\n",
    "        print(f'Labeling {len(input_list)} instances')\n",
    "\n",
    "    # For instance in input_list:\n",
    "    # - The textual sentence is given by instance['Text']\n",
    "    # - Give for each of the 19 values a confidence for 'attained', 'constrined', and 'none'.\n",
    "    #   As these confidences have to add up to 1.0, the value for 'none' can be omitted\n",
    "    return [\n",
    "        {values[i] + k: v for i in range(len(values)) for k, v in zip((' attained', ' constrained'), choose_values_by_probability(probabilities_resorted[i], probabilities_attained[i]))} for _ in input_list\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classification on TIRA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# \"instance\" is a dict with keys \"Text-ID\", \"Sentence\", and \"Text\"\n",
    "def labelInstances(instances: List[Dict]):\n",
    "   predictions = [{\"Text-ID\": instance[\"Text-ID\"], \"Sentence\": instance[\"Sentence\"], **labels} for instance, labels in zip(instances, predict(instances))]\n",
    "   return predictions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def writeRun(labels, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    df = pd.DataFrame.from_dict(labels)\n",
    "\n",
    "    print(\"Writing run file\")\n",
    "    output_file = os.path.join(output_dir, \"predictions.tsv\")\n",
    "    df.to_csv(output_file, header=True, index=False, sep='\\t')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not runs_as_inference_server:\n",
    "    input_file = os.path.join(dataset_dir, \"sentences.tsv\")\n",
    "    writeRun(labelInstances(pd.read_csv(input_file, sep='\\t', header=0, index_col=None).to_dict('records')), output_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
